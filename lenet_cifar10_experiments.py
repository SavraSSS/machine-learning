# -*- coding: utf-8 -*-
"""ТИМО3 Саврасов ПА

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yDZEfmsyrbsplOL35_dKTVsxlTUxrzfr

#Предобработка данных
"""

import torch
import pandas as pd
import numpy as np
import random
import torchvision.datasets

random.seed(0)
np.random.seed(0)
torch.cuda.manual_seed(0)
torch.backends.cudnn.deterministic = True

CIFAR_train=torchvision.datasets.CIFAR10(root='./', train=True, download=True)
CIFAR_test=torchvision.datasets.CIFAR10(root='./', train=False, download=True)

X_train = CIFAR_train.data
y_train = CIFAR_train.targets
X_test = CIFAR_test.data
y_test = CIFAR_test.targets

X_train = np.moveaxis(X_train, 3, 1)
X_test = np.moveaxis(X_test, 3, 1)

X_train.shape

X_test.shape

len(y_train)

len(y_test)

X_train = X_train.astype(np.float32)/255
X_test = X_test.astype(np.float32)/255

X_train = torch.from_numpy(X_train)
y_train = torch.as_tensor(y_train)
X_test = torch.from_numpy(X_test)
y_test = torch.as_tensor(y_test)

import matplotlib.pyplot as plt
fig, axes = plt.subplots(1, 4)

axes[0].imshow(np.transpose(X_train[6,:,:,:], (1,2,0)))
axes[0].set_title("6")
axes[0].set_xticks([])
axes[0].set_yticks([])

axes[1].imshow(np.transpose(X_train[7,:,:,:], (1,2,0)))
axes[1].set_title("7")
axes[1].set_xticks([])
axes[1].set_yticks([])

axes[2].imshow(np.transpose(X_train[8,:,:,:], (1,2,0)))
axes[2].set_title("8")
axes[2].set_xticks([])
axes[2].set_yticks([])

axes[3].imshow(np.transpose(X_train[9,:,:,:], (1,2,0)))
axes[3].set_title("9")
axes[3].set_xticks([])
axes[3].set_yticks([])

plt.show()

"""#LeNet"""

class LeNet(torch.nn.Module):
  def __init__(self):
    super (LeNet, self).__init__()

    self.conv1 = torch.nn.Conv2d(
        in_channels = 3, out_channels=6, kernel_size = 5, padding = 2)
    self.act1 = torch.nn.Tanh()
    self.pool1 = torch.nn.AvgPool2d(kernel_size= 2, stride = 2)
    self.conv2 = torch.nn.Conv2d(
        in_channels = 6, out_channels=16, kernel_size = 5, padding = 0)
    self.act2 = torch.nn.Tanh()
    self.pool2 = torch.nn.AvgPool2d(kernel_size= 2, stride = 2)

    self.fc1 = torch.nn.Linear(6*6*16,120)
    self.act3 = torch.nn.Tanh()

    self.fc2 = torch.nn.Linear(120,84)
    self.act4 = torch.nn.Tanh()

    self.fc3 = torch.nn.Linear(84,10)

  def forward(self,x):
    x = self.conv1(x)
    x = self.act1(x)
    x = self.pool1(x)

    x = self.conv2(x)
    x = self.act2(x)
    x = self.pool2(x)

    x = x.view(x.size(0), x.size(1)* x.size(2) * x.size(3))

    x = self.fc1(x)
    x = self.act3(x)
    x = self.fc2(x)
    x = self.act4(x)
    x = self.fc3(x)

    return x

net = LeNet()

device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
net = net.to(device)

loss = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.RMSprop(net.parameters(), lr=0.001, weight_decay=0.01)

batch_size = 30

test_acc = []
test_loss = []

X_test = X_test.to(device)
y_test = y_test.to(device)

for e in range(30):
  order = np.random.permutation(len(X_train))
  for i in range(0, len(X_train), batch_size):
    optimizer.zero_grad()

    batch_ind = order[i:i+batch_size]

    X_batch = X_train[batch_ind].to(device)
    y_batch = y_train[batch_ind].to(device)
    preds = net.forward(X_batch)
    loss_val = loss(preds, y_batch)
    loss_val.backward()

    optimizer.step()

  with torch.no_grad():
    test_preds = net.forward(X_test)
    test_loss.append(loss(test_preds, y_test).data.cpu())

  print(test_loss[-1])

"""#ReLu"""

class LeNetReLU(torch.nn.Module):
  def __init__(self):
    super (LeNetReLU, self).__init__()

    self.conv1 = torch.nn.Conv2d(
        in_channels = 3, out_channels=6, kernel_size = 5, padding = 2)
    self.act1 = torch.nn.ReLU()
    self.pool1 = torch.nn.AvgPool2d(kernel_size= 2, stride = 2)
    self.conv2 = torch.nn.Conv2d(
        in_channels = 6, out_channels=16, kernel_size = 5, padding = 0)
    self.act2 = torch.nn.ReLU()
    self.pool2 = torch.nn.AvgPool2d(kernel_size= 2, stride = 2)

    self.fc1 = torch.nn.Linear(6*6*16,120)
    self.act3 = torch.nn.ReLU()

    self.fc2 = torch.nn.Linear(120,84)
    self.act4 = torch.nn.ReLU()

    self.fc3 = torch.nn.Linear(84,10)

  def forward(self,x):
    x = self.conv1(x)
    x = self.act1(x)
    x = self.pool1(x)

    x = self.conv2(x)
    x = self.act2(x)
    x = self.pool2(x)

    x = x.view(x.size(0), x.size(1)* x.size(2) * x.size(3))

    x = self.fc1(x)
    x = self.act3(x)
    x = self.fc2(x)
    x = self.act4(x)
    x = self.fc3(x)

    return x

net = LeNetReLU()

device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
net = net.to(device)

loss = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.RMSprop(net.parameters(), lr=0.001, weight_decay=0.01)

batch_size = 30

test_loss_ReLU = []

X_test = X_test.to(device)
y_test = y_test.to(device)

for e in range(30):
  order = np.random.permutation(len(X_train))
  for i in range(0, len(X_train), batch_size):
    optimizer.zero_grad()

    batch_ind = order[i:i+batch_size]

    X_batch = X_train[batch_ind].to(device)
    y_batch = y_train[batch_ind].to(device)
    preds = net.forward(X_batch)
    loss_val = loss(preds, y_batch)
    loss_val.backward()

    optimizer.step()

  with torch.no_grad():
    test_preds = net.forward(X_test)
    test_loss_ReLU.append(loss(test_preds, y_test).data.cpu())

  print(test_loss_ReLU[-1])

"""#MaxPool"""

class LeNetMaxPool(torch.nn.Module):
  def __init__(self):
    super (LeNetMaxPool, self).__init__()

    self.conv1 = torch.nn.Conv2d(
        in_channels = 3, out_channels=6, kernel_size = 5, padding = 2)
    self.act1 = torch.nn.ReLU()
    self.pool1 = torch.nn.MaxPool2d(kernel_size= 2, stride = 2)
    self.conv2 = torch.nn.Conv2d(
        in_channels = 6, out_channels=16, kernel_size = 5, padding = 0)
    self.act2 = torch.nn.ReLU()
    self.pool2 = torch.nn.MaxPool2d(kernel_size= 2, stride = 2)

    self.fc1 = torch.nn.Linear(6*6*16,120)
    self.act3 = torch.nn.ReLU()

    self.fc2 = torch.nn.Linear(120,84)
    self.act4 = torch.nn.ReLU()

    self.fc3 = torch.nn.Linear(84,10)

  def forward(self,x):
    x = self.conv1(x)
    x = self.act1(x)
    x = self.pool1(x)

    x = self.conv2(x)
    x = self.act2(x)
    x = self.pool2(x)

    x = x.view(x.size(0), x.size(1)* x.size(2) * x.size(3))

    x = self.fc1(x)
    x = self.act3(x)
    x = self.fc2(x)
    x = self.act4(x)
    x = self.fc3(x)

    return x

net = LeNetMaxPool()

device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
net = net.to(device)

loss = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.RMSprop(net.parameters(), lr=0.001, weight_decay=0.01)

batch_size = 30

test_loss_MaxPool = []

X_test = X_test.to(device)
y_test = y_test.to(device)

for e in range(30):
  order = np.random.permutation(len(X_train))
  for i in range(0, len(X_train), batch_size):
    optimizer.zero_grad()
    batch_ind = order[i:i+batch_size]
    X_batch = X_train[batch_ind].to(device)
    y_batch = y_train[batch_ind].to(device)
    preds = net.forward(X_batch)
    loss_val = loss(preds, y_batch)
    loss_val.backward()
    optimizer.step()
  with torch.no_grad():
    test_preds = net.forward(X_test)
    test_loss_MaxPool.append(loss(test_preds, y_test).data.cpu())

  print(test_loss_MaxPool[-1])

"""#Upd"""

class LeNetUpd(torch.nn.Module):
  def __init__(self):
    super (LeNetUpd, self).__init__()

    self.conv1_1 = torch.nn.Conv2d(
                in_channels=3, out_channels=6, kernel_size=3, padding=0)
    self.conv1_2 = torch.nn.Conv2d(
                in_channels=6, out_channels=6, kernel_size=3, padding=0)

    self.conv2_1 = torch.nn.Conv2d(
                in_channels=6, out_channels=16, kernel_size=3, padding=0)
    self.conv2_2 = torch.nn.Conv2d(
                in_channels=16, out_channels=16, kernel_size=3, padding=0)

    self.act1 = torch.nn.ReLU()
    self.pool1 = torch.nn.MaxPool2d(kernel_size= 2, stride = 2)

    self.act2 = torch.nn.ReLU()
    self.pool2 = torch.nn.MaxPool2d(kernel_size= 2, stride = 2)

    self.fc1 = torch.nn.Linear(5*5*16,120)
    self.act3 = torch.nn.ReLU()

    self.fc2 = torch.nn.Linear(120,84)
    self.act4 = torch.nn.ReLU()

    self.fc3 = torch.nn.Linear(84,10)

  def forward(self,x):
    x = self.conv1_2(self.conv1_1(x))
    x = self.act1(x)
    x = self.pool1(x)

    x = self.conv2_2(self.conv2_1(x))
    x = self.act2(x)
    x = self.pool2(x)

    x = x.view(x.size(0), x.size(1)* x.size(2) * x.size(3))

    x = self.fc1(x)
    x = self.act3(x)
    x = self.fc2(x)
    x = self.act4(x)
    x = self.fc3(x)

    return x

net = LeNetUpd()

device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
net = net.to(device)

loss = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)

batch_size = 30

test_loss_Upd = []

X_test = X_test.to(device)
y_test = y_test.to(device)

for e in range(30):
  order = np.random.permutation(len(X_train))
  for i in range(0, len(X_train), batch_size):
    optimizer.zero_grad()
    batch_ind = order[i:i+batch_size]
    X_batch = X_train[batch_ind].to(device)
    y_batch = y_train[batch_ind].to(device)
    preds = net.forward(X_batch)
    loss_val = loss(preds, y_batch)
    loss_val.backward()
    optimizer.step()
  with torch.no_grad():
    test_preds = net.forward(X_test)
    test_loss_Upd.append(loss(test_preds, y_test).data.cpu())

  print(test_loss_Upd[-1])

plt.plot(test_loss, label='LeNet')
plt.plot(test_loss_ReLU, label='ReLU')
plt.plot(test_loss_MaxPool, label='MaxPool')
plt.plot(test_loss_Upd, label='Upd')

plt.title('Loss train')
plt.legend()
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.show()

def first(inp,k,s,p,out):
  w=(inp[0]-k[0]+2*p[0])//s[0]+1
  h=(inp[1]-k[1]+2*p[1])//s[1]+1
  return out,w,h

res = first((1024,1024),(5,5),(1,1), (1,1), 44)
res

res = first((128,128),(9,9),(1,1), (1,1), 41)
res

res = first((64,64),(9,9),(1,1), (1,1), 41)
res

def second(w,h,d,k):
  return ((w*h*d)+1)*k

res = second(7,7,4,4) + second(5,5,4,6)
res

res = second(9,9,4,4)+second(9,9,4,6)+second(9,9,6,7)+second(5,5,7,8)
res

res = second(9,9,4,5) + second(3,3,5,8)
res